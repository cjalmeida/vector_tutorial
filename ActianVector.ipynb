{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACTIAN VECTOR TUTORIAL\n",
    "============\n",
    "\n",
    "In this post, we're going to show you how to install and run big data analytics using [Actian Vector](https://www.actian.com/analytic-database/vector-smp-analytic-database/) columnar database using Jupyter Notebooks. Compared to traditional row-oriented databases, columnar databases like Vector really shine on analytics tasks where you need to do a lot of aggregate operations across many rows. Actian Vector in particular leverage modern processor architecture (multi-core and SIMD instructions) to heavily paralellize the workload and speed-up calculations.\n",
    "\n",
    "Installing Actian Vector\n",
    "------------------------\n",
    "\n",
    "We'll work in a *Ubuntu Linux 64-bit 16.06 LTS* machine. We'll also assume you're using `bash`. Vector should work on any recent Linux distro so adapt the instructions below as needed. To install Actian Vector, we're going to follow the \"Getting Started\" section of the [official documentation](http://docs.actian.com/vector/5.0/index.html). First install the system dependencies, namely *LibAIO*\n",
    "\n",
    "```bash\n",
    "sudo apt install libaio1\n",
    "```\n",
    "\n",
    "First, download [Actian Vector Community Edition] (https://www.actian.com/lp/vector-community-edition/) that allows you 1TB of data without any time limit. After receiving the download link by email and fetching the install `tgz` file, uncompress and run the `install.sh` as super-user then follow instructions.\n",
    "\n",
    "```bash\n",
    "tar xvf ~/Downloads/actian-vector-5.0.0-405-community-linux-x86_64.tgz\n",
    "cd actian-vector-5.0.0-405-community-linux-x86_64/\n",
    "sudo ./install.sh\n",
    "```\n",
    "\n",
    "The default params will install under `/opt/Actian/VectorVW` (server ID `VW`) and create an `actian` user on your machine. Now let's run some post-installation commands to setup Vector as a system service.\n",
    "\n",
    "```bash\n",
    "sudo -u actian bash -c 'source ~actian/.ingVWsh && mkrc'\n",
    "sudo cp /opt/Actian/VectorVW/ingres/files/rcfiles/actian-vectorVW /etc/init.d/\n",
    "sudo systemctl enable actian-vectorVW\n",
    "sudo systemctl start actian-vectorVW\n",
    "\n",
    "```\n",
    "\n",
    "Now you can use the regular `systemctl` commands to `start` and `stop` your `actian-vectoVW` service. Most administrative commands are run under the created `actian` user. We'll also `source` a small shell script (created during install) that setup `PATH` and other enviroment variables. Use the command below to run Vector commands as `actian` user.\n",
    "\n",
    "```bash\n",
    "sudo -u -i actian\n",
    "source ~/.ingVWsh\n",
    "\n",
    "```\n",
    "\n",
    "To test your server is up and running, run the command below as `actian` user:\n",
    "\n",
    "```bash\n",
    "echo 'select 1\\g' | sql iidbdb\n",
    "```\n",
    "\n",
    "This should output something like:\n",
    "\n",
    "```\n",
    "* Executing . . .\n",
    "\n",
    "\n",
    "┌──────┐\n",
    "│col1  │\n",
    "├──────┤\n",
    "│     1│\n",
    "└──────┘\n",
    "(1 row)\n",
    "continue\n",
    "* \n",
    "Your SQL statement(s) have been committed.\n",
    "```\n",
    "If you get an error like `E_LQ0001 Failed to connect to DBMS session.`, it means your server is not running or something went wrong during installation.\n",
    "\n",
    "\n",
    "Creating a new test database \n",
    "----------------------------\n",
    "\n",
    "We're going to use the `createdb` command to create a new `testdb` database and setup ODBC connection.\n",
    "\n",
    "```bash\n",
    "## as \"actian\" user\n",
    "createdb testdb\n",
    "iiodbcadmin add testdb\n",
    "iiodbcadmin test testdb\n",
    "```\n",
    "\n",
    "The last command should output `iiodbcadmin: Connection was successful.` Now we're going to grant permission on the DB to your user using the SQL `CREATE USER` and `GRANT` statements. In the SQL commands below, replace `<user>` with the name of the OS user account that will run the Jupyter Notebook.\n",
    "\n",
    "```bash\n",
    "## as \"actian\" user\n",
    "echo 'CREATE USER <user> WITH NOPROFILE, NOGROUP, NOEXPIRE_DATE, NOSECURITY_AUDIT \\g' | sql iidbdb\n",
    "echo 'GRANT ALL PRIVILEGES ON DATABASE testdb TO <user> \\g' | sql iidbdb\n",
    "```\n",
    "\n",
    "\n",
    "Installing Python dependencies\n",
    "------------------------------\n",
    "\n",
    "If you don't already have a *Python 3.6* environment ready, install one using *Anaconda* package manager. Download Anaconda (or Miniconda if you prefer) from the [official download site](https://www.continuum.io/downloads). After download just run the command below and follow the instructions:\n",
    "\n",
    "```bash\n",
    "bash Anaconda3-4.4.0-Linux-x86_64.sh\n",
    "```\n",
    "\n",
    "The default (and assumed) install location is `~/anaconda3`. We're going to create a new virtual environment named `vector` just for this tutorial and activate it:\n",
    "\n",
    "```bash\n",
    "~/anaconda3/bin/conda create -y -n vector\n",
    "source ~/anaconda3/bin/activate vector\n",
    "```\n",
    "\n",
    "This will give you a Bash prompt with `(vector)` string prepended. Now install the `conda` dependencies by running the command below:\n",
    "\n",
    "```bash\n",
    "## in (vector) virtualenv\n",
    "conda install -y jupyter pandas pyodbc matplotlib seaborn tqdm\n",
    "conda install -c conda-forge turbodbc=2.0.0\n",
    "```\n",
    "\n",
    "Running (this) Jupyter Notebook\n",
    "-----------------------------------\n",
    "\n",
    "Before running the Jupyter server, we need setup the environment variables as the user that will run the Jupyter notebook.\n",
    "\n",
    "```bash\n",
    "source ~actian/.ingVWsh\n",
    "export ODBCSYSINI=/opt/Actian/VectorVW/ingres/files\n",
    "\n",
    "## test we can connect to Vectian as the non-admin user\n",
    "iiodbcadmin test testdb\n",
    "```\n",
    "\n",
    "Activate the `vector` virtual environment we created before, start the Jupyter server and open this `ActianVector.ipynb` notebook. \n",
    "\n",
    "```bash\n",
    "## in the directory containing the .ipynb file and code\n",
    "source ~/anaconda3/bin/activate vector\n",
    "\n",
    "## in (vector) virtualenv\n",
    "jupyter notebook .\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecting to Vector using turbodbc\n",
    "---------------------------------\n",
    "\n",
    "The `turbodbc` is a DBAPI2 compatible Python library to connect to datasources providing an ODBC driver. The code below will connect to Vector and issue a sample SQL statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: [1]\n"
     ]
    }
   ],
   "source": [
    "import turbodbc as odbc\n",
    "\n",
    "# Connect to the pre-configured data source\n",
    "conn = odbc.connect('testdb')\n",
    "\n",
    "# Create a cursor and execute a statement.\n",
    "cursor = conn.cursor()\n",
    "cursor.execute('SELECT 1 as test_col')\n",
    "res = cursor.fetchone()\n",
    "\n",
    "print('Result:', res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Case: Smart energy metering logs\n",
    "------------------------------------\n",
    "\n",
    "Our test data will simulating a fleet of smart energy meters sending daily records consisting of energy consumption (in kWh), max and min voltage levels (in V) and a flag indicating an outage/brown-out. \n",
    "\n",
    "### Schema creation\n",
    "The data is modeled after a typical \"Star Schema\" with a `metering_fact` fact table, and `customer_dim` and `date_dim` dimensions, as shown by the diagram below:\n",
    "\n",
    "<p>\n",
    "<img src=\"files/schema.png\">\n",
    "<p>\n",
    "\n",
    "Let's create the schema now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_dim_ddl = \"\"\"\n",
    "create table date_dim(\n",
    "    skey integer4,\n",
    "    date ansidate,\n",
    "    day integer1,\n",
    "    month integer1,\n",
    "    year integer2,\n",
    "    day_of_week integer1,\n",
    "    week integer1,\n",
    "    quarter integer1\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "customer_dim_ddl = \"\"\"\n",
    "create table customer_dim(\n",
    "    skey integer4,\n",
    "    zipcode integer4,\n",
    "    county varchar(255),\n",
    "    state varchar(2)\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "metering_fact_ddl = \"\"\"\n",
    "create table metering_fact(\n",
    "    customer_skey integer4,\n",
    "    date_skey integer4,\n",
    "    consumption integer4,\n",
    "    min_voltage integer2,\n",
    "    max_voltage integer2,\n",
    "    outage integer1\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(date_dim_ddl)\n",
    "cursor.execute(customer_dim_ddl)\n",
    "cursor.execute(metering_fact_ddl)\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not using any sort of index or constraint in the data model. First, Vector has it's own optimization for anaytical queries that generally doesn't require explict indexes. As for constraints, they have a performance impact on load and it's common to delegate data integrity to the ETL process instead of having it on the database on anaytical workloads.\n",
    "\n",
    "### Data loading\n",
    "\n",
    "The \"customer\" data is sampled randomly from a list of zip codes from the \"New England\" region of the US (Maine, Vermont, New Hampshire, Massachusetts, Rhode Island, and Connecticut) until we get about 500 thousand \"customers\". Then we generate daily measurements for each customer for two years (2015 and 2016) giving us about 356 million rows. The measurements are generated from a gaussian distribution with higher average consumption in winter and summer months.\n",
    "\n",
    "The actual implementation code is in the `vector_tutorial.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0956fc9430a84d848a556d47e27f2646"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae53491714447ec9680fc5f406f18d5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3916ba7dd114092b8227712f38396ec"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c233d55da873>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvector_tutorial\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/work/tc/actian/vector_tutorial.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(conn)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustomer_skey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_skey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconsumption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_voltage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_voltage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0mcommit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/tc/actian/vector_tutorial.py\u001b[0m in \u001b[0;36mcommit\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcommit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutemany\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/vector/lib/python3.6/site-packages/turbodbc/exceptions.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInternError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mDatabaseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/vector/lib/python3.6/site-packages/turbodbc/cursor.py\u001b[0m in \u001b[0;36mexecutemany\u001b[0;34m(self, sql, parameters)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mparameter_set\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameter_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from vector_tutorial import load_data\n",
    "load_data(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
